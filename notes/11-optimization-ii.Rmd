---
title: Optimization II
author: Charlotte Wickham
date: Oct 30th 2018
---

```{r setup, message = FALSE}
library(tidyverse)
library(here)
set.seed(36528)
```

```{r, include = FALSE}
true_pi <- 0.8
true_mu <- c(0,   2)
true_sigma <- c(1,   0.5)
n <- 250
pop <- rbinom(n, size = 1, prob = 1 - true_pi) + 1
x <- rnorm(n, mean = true_mu[pop], sd = true_sigma[pop])
```


# Newton-Raphson

*(Moved from last Thursday's lecture)*

Usually described as a **root** finding method, e.g. find $x$ such that $f(x) = 0$.

Can transform to a minimization method by applying to the derivative $f'()$, and checking point is a minimum.

**As a root finding method**

Assume we have $x_0$ a value that is close to the root.

Apply Taylor expansion to approximate $f()$:
$$
f(x) \approx f(x_0) + (x - x_0)f'(x_0)
$$

Use this approximation to find an approximate root:
$$
f(x_0) + (x - x_0)f'(x_0) = 0 \\
\implies x = x_0 - \frac{f(x_0)}{f'(x_0)}
$$

*Charlotte to draw picture here*

**Root finding algorithm**

1. Start with a value near the root, $x_0$

2. Generate better values iteratively $x_{i + 1} = x_{i} - \frac{f(x_i)}{f'(x_i)}$

3. Stop when $x_{i + 1}$ isn't very different from $x_i$


**Minimization algorithm**

1. Start with a value near the minimum, $x_0$

2. Generate better values iteratively $x_{i + 1} = x_{i} - \frac{f'(x_i)}{f''(x_i)}$

3. Stop when $x_{i + 1}$ isn't very different from $x_i$, or $f'(x_i)$ is below some small number, or if $i$ reaches some maximum number of iterations (failure to converge).


## Multivariate version

Start with a guess $\mathbf{x}_0$ (a $p \times 1$ vector), then iteratively update:
$$
\mathbf{x}_{i + 1} = \mathbf{x}_{i} - \mathbf{H}(\mathbf{x_i})^{-1}\nabla f(\mathbf{x}_{i})
$$

Where $\nabla f()_{p \times 1}$ is the gradient vector and $H()_{p \times p}$ is the Hessian matrix.

In GLM, if you replace Hessian with expected value of Hessian you get something called Fisher Scoring, also know as iteratively weighted least squares.


## Example: Two Normal Mixture 

Consider the following sample, $n = 250$

```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(binwidth = 0.25)
```

One model might be a two component Normal mixture: arises when with probability $\pi$ we get a value from N$(\mu_1, \sigma_1^2)$, and otherwise a sample from N$(\mu_2, \sigma_2^2)$.

$$
f(x; \pi, \mu_1, \mu_2, \sigma_1, \sigma_2) = \pi\phi(x; \mu_1, \sigma_1^2) +  (1- \pi)\phi(x; \mu_2, \sigma_2^2) 
$$
where $\phi(x; \mu, \sigma^2)$ is the density function for a Normal($\mu, \sigma^2)$.

## Your Turn

Let $\theta = (\pi, \mu_1, \mu_2, \sigma_1, \sigma_2)$.

Fill in the body of the function to calculate the density function for a specified `theta` and data vector `x`:
```{r}
dmix <- function(x, theta){
  pi <- theta[1]
  mu_1 <- theta[2]
  mu_2 <- theta[3]
  sigma_1 <- theta[4]
  sigma_2 <- theta[5]
  pi * dnorm(x, mean = mu_1, sd = sigma_1) +
   (1 - pi) * dnorm(x, mean = mu_2, sd = sigma_2) 
} 
dmix(0, c(0, 1, 1, 1, 1))
```

Then try some more reasonable parameters to compare to the data:
```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(aes(y = stat(density)), binwidth = 0.25) +
  stat_function(fun = dmix, args = list(theta = c(0.6, 0.2, 2, 1.5, 1)))
```


## Find MLE with maximization

Need negative log likelihood:
```{r}
nllhood_mix <- function(theta, x){
  d <- dmix(x, theta)
  -1 * sum(log(d))
}
```

`optim()` with `method = "BFGS"`, does a **Quasi** Newton method (swaps Hessian with an approximation).


**Your Turn** Provide starting values to the `par` argument: `par` is like the $\mathbb{x}_0$ above, values for $\theta$ we think are close to the values that give the minimum:

## Things that go wrong

* **Lot's of messages** - bad parameter values lead to messages with `NaN`
* **Non convergence** no convergence due to bad parameter values
* **Local minima** 

## Lot's of messages 

This converges, `$convergence` is `0`, but we get a lot of messages about `NaN`.  This happens when a call to `nllhood_mix()` returns `NaN`, mostly when using bad values for the parameters, e.g. $\pi = -1$, or $\sigma_1 < 0$.

```{r, warning = FALSE}
(mle1 <- optim(par = c(1, 0, 1.8, 1.2, 0.7), fn = nllhood_mix, x = x, method = "BFGS"))
```

`optim()` can handle this fine, so we don't really need to worry.

To avoid `NaN` warnings we could protect our function from returning values at bad parameter values:
```{r}
nllhood_mix2 <- function(theta, x){
  d <- dmix(x, theta)
  if (any(d < 0) || any(is.nan(d))){
    return(Inf)
  }
  -1 * sum(log(d))
}
```

But this does introduce some weird discontinuities in our function.

```{r, warning = FALSE, message = FALSE}
optim(par = c(0.4, 2, -0.2, 1, 2), fn = nllhood_mix2, x = x, 
  method = "BFGS")
```

Or use a bounded algorithm (see below).

## Non-convergence

This doesn't converge, `$convergence` is `1`,:
```{r, warning = FALSE, message=FALSE}
(mle2 <- optim(par = c(0.5, 0, 1, 2, 1), fn = nllhood_mix, x = x, method = "BFGS"))
```

This means we hit our maximum number of iterations, we can start again from where we finished:

```{r, message=FALSE, warning = FALSE}
optim(par = mle2$par, fn = nllhood_mix, x = x, method = "BFGS")
```

But we still don't converge.  In this case, our likelihood rapidly decreases away from the parameter space. 

## Local minima

These starting parameters converge (with reasonable values of parameters) but not to the global minimum:
```{r, message=FALSE, warning = FALSE}
(mle3 <- optim(par = c(0.5, 1, 1, 1, 1), fn = nllhood_mix, x = x, method = "BFGS"))
mle3$par
```

Notice the value of the negative log likelihood, `r round(mle3$value, 1)`, is bigger than that at the other minimum `r round(mle1$value, 1)`.  This is a local minimum.

**Always try a few values for the starting parameters**.

## Our MLE fit

```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(aes(y = stat(density)), binwidth = 0.25) +
  stat_function(fun = dmix, args = list(theta = mle1$par))
```

## Derivatives

We didn't supply the gradient -- `optim()` will numerically approximate it with a finite difference.

> Practical issues with general optimzation often have less to do with the optimizer than with how carefully the problem is set up.  

> In general it is worth supplying a function to calculate derivatives if you can, although it may be quicker in a one-off problem to let the software calculate numerical derivatives.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).

You can do it "by hand", or for some simple functions (include things like `dnorm()`) get R to do it, see `?deriv`.

## Scaling

> It is worth ensuring the problem is reasonably well scaled, so a unit step in any parameter can have a comparable change in size to the objective, preferably about a unit change at the optimium.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).

**Your Turn**

Using `nllhood()` experiment with some values of `theta`, how much does the likelihood change for a unit change in `pi`?  Compare it to a unit change in `mu_1`.

```{r, eval = FALSE}
nllhood_mix(theta = c(0.01, -0.2, 2, 2, 1), x = x)
```
```{r, eval = FALSE}
nllhood_mix(theta = c(0.09, -0.2, 2, 2, 1), x = x)
```

```{r, eval = FALSE}
nllhood_mix(theta = c(0.5, -0.2, 2, 2, 1), x = x)
```

```{r, eval = FALSE}
nllhood_mix(theta = c(0.5, 0.8, 2, 2, 1), x = x)
```

*(It will depend on the values of the other parameters, so try some near the values you would guess from the plot)*

You can specify a scaling vector with `control = list(parscale = c())`, "Optimization is performed on `par/parscale`":
```{r, warning = FALSE}
optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, 
  method = "BFGS",
  control = list(parscale = c(5, 1, 1, 1, 1)))
```

## MLE and the Hessian

If $\hat{\theta}$ is the MLE for $\theta$, then for large $n$,
$$
\hat{\theta} \dot{\sim} N(\theta, I(\hat{\theta})^{-1})
$$

where $I(\hat{\theta})$ is the observed Fisher Information matrix, the negative Hessian of the likelihood evaluated at the MLE.

Or, in other words, the standard errors for the estimates are the square root of the diagonal of the inverse observed Fisher Information.

Since we minimized the negative log likelihood, the Hessian returned by `optim()` **is** the observed Fisher information matrix.

```{r, warning = FALSE}
(mle <- optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, method = "BFGS",
  hessian = TRUE))
mle$hessian %>% solve() %>% diag() %>% sqrt()
```

```{r}
data.frame(
  est = mle$par, 
  se = mle$hessian %>% solve() %>% diag() %>% sqrt()
)
```

## Bounds on parameters

Use `method = "L-BFGS-B"` and specify `lower` and `upper`.

# Nelder Mead

`optim()` with default method.

No need for derivatives...can be slower to converge.

**Idea**: Evaluate function at a special arrangement of points (a simplex), then consider possible changes to the arrangement:

* Reflection
* Expansion
* Contraction
* Shrink

Nice animation: https://www.benfrederickson.com/numerical-optimization/




