---
title: Optimization II
author: Charlotte Wickham
date: Oct 30th 2018
---

```{r setup, message = FALSE}
library(tidyverse)
library(here)
set.seed(36528)
```

```{r, include = FALSE}
true_pi <- 0.8
true_mu <- c(0,   2)
true_sigma <- c(1,   0.5)
n <- 250
pop <- rbinom(n, size = 1, prob = 1 - true_pi) + 1
x <- rnorm(n, mean = true_mu[pop], sd = true_sigma[pop])
```


# Newton-Raphson

*(Moved from last Thursday's lecture)*

Usually described as a **root** finding method, e.g. find $x$ such that $f(x) = 0$.

Can transform to a minimization method by applying to the derivative $f'()$, and checking point is a minimum.

**As a root finding method**

Assume we have $x_0$ a value that is close to the root.

Apply Taylor expansion to approximate $f()$:
$$
f(x) \approx f(x_0) + (x - x_0)f'(x_0)
$$

Use this approximation to find an approximate root:
$$
f(x_0) + (x - x_0)f'(x_0) = 0 \\
\implies x = x_0 - \frac{f(x_0)}{f'(x_0)}
$$

*Charlotte to draw picture here*

**Root finding algorithm**

1. Start with a value near the root, $x_0$

2. Generate better values iteratively $x_{i + 1} = x_{i} - \frac{f(x_i)}{f'(x_i)}$

3. Stop when $x_{i + 1}$ isn't very different from $x_i$


**Minimization algorithm**

1. Start with a value near the minimum, $x_0$

2. Generate better values iteratively $x_{i + 1} = x_{i} - \frac{f'(x_i)}{f''(x_i)}$

3. Stop when $x_{i + 1}$ isn't very different from $x_i$, or $f'(x_i)$ is below some small number, or if $i$ reaches some maximum number of iterations (failure to converge).


## Multivariate version

Start with a guess $\mathbf{x}_0$ (a $p \times 1$ vector), then iteratively update:
$$
\mathbf{x}_{i + 1} = \mathbf{x}_{i} - \mathbf{H}(\mathbf{x_i})^{-1}\nabla f(\mathbf{x}_{i})
$$

Where $\nabla f()_{p \times 1}$ is the gradient vector and $H()_{p \times p}$ is the Hessian matrix.

In GLM, if you replace Hessian with expected value of Hessian you get something called Fisher Scoring, also know as iteratively weighted least squares.


## Example: Two Normal Mixture 

Consider the following sample, $n = 250$

```{r}
ggplot(mapping = aes(x = x)) +
  geom_histogram(binwidth = 0.25)
```

One model might be a two component Normal mixture: arises when with probability $\pi$ we get a value from N$(\mu_1, \sigma_1^2)$, and otherwise a sample from N$(\mu_2, \sigma_2^2)$.

$$
f(x; \pi, \mu_1, \mu_2, \sigma_1, \sigma_2) = \pi\phi(x; \mu_1, \sigma_1^2) +  (1- \pi)\phi(x; \mu_2, \sigma_2^2) 
$$
where $\phi(x; \mu, \simga^2)$ is the density function for a Normal($\mu, \sigma^2)$.

## Your Turn

Let $\theta = (\pi, \mu_1, \mu_2, \sigma_1, \sigma_2)$.

Fill in the body of the function to calculate the density function for a specified `theta` and data vector `x`:
```{r, eval = FALSE}
dmix <- function(x, theta){
  
} 
```

Then try some more reasonable parameters to compare to the data:
```{r, eval = FALSE}
ggplot(mapping = aes(x = x)) +
  geom_histogram(aes(y = stat(density)), binwidth = 0.25) +
  stat_function(fun = dmix, args = list(theta = c(0.5, -2, 2, 1, 1)))
```


## Find MLE with maximization

Need negative log likelihood:
```{r}
nllhood_mix <- function(theta, x){
  -1 * sum(log(dmix(x, theta)))
}
```

`optim()` with `method = "BFGS"`, does a **Quasi** Newton method (swaps Hessian with an approximation).


**Your Turn** Provide starting values to the `par` argument:

```{r, eval = FALSE}
(mle1 <- optim(par = c(), fn = nllhood_mix, x = x, method = "BFGS"))
```

## Things that go wrong

```{r, warning = FALSE}
dmix <- function(x, theta){
  pi <- theta[1]
  mu <- theta[2:3]
  sigma <- theta[4:5]
  pi * dnorm(x, mean = mu[1], sd = sigma[1]) + 
   (1 - pi) * dnorm(x, mean = mu[2], sd = sigma[2])
} 
optim(par = c(0.01, 1, 1, 4, 4), fn = nllhood_mix, x = x, method = "BFGS")
```

*Talk about adding some bounds in function*

```{r, warning = FALSE}
(mle <- optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, method = "BFGS"))
```

## Derivatives

We didn't supply the gradient -- `optim()` will numerically approximate it with a finite difference.

> Practical issues with general optimzation often have less to do with the optimizer than with how carefully the problem is set up.  

> In general it is worth supplying a function to calculate derivatives if you can, although it may be quicker in a one-off problem to let the software calculate numerical derivatives.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).

You can do it "by hand", or for some simple functions get R to do it, see `?deriv`.

## Scaling

> It is worth ensuring the problem is reasonably well scaled, so a unit step in any parameter can have a comparable cahange in size to the objective, preferably about a unit change at the optimium.

-- Ripley, B. D. "Modern applied statistics with S." Statistics and Computing, fourth ed. Springer, New York (2002).

**Your Turn**

Using `nllhood()` experiment with some values of `theta`, how much does the likelihood change for a unit change in `pi`?  Compare it to a unit change in `mu_1`.

```{r, eval = FALSE}
nllhood_mix(theta = c(), x = x)
```

*(It will depend on the values of the other parameters, so try some near the values you would guess from the plot)*

You can specify a scaling vector with `control = list(parscale = c())`, "Optimization is performed on `par/parscale`":
```{r, warning = FALSE}
optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, method = "BFGS",
  control = list(parscale = c(10, 1, 1, 1, 1)))
```

## MLE and the Hessian

If $\hat{\theta}$ is the MLE for $\theta$, then for large $n$,
$$
\hat{\theta} \dot{\sim} N(\theta, I(\hat{\theta})^{-1})
$$

where $I(\hat{\theta})$ is the observed Fisher Information matrix, the negative Hessian of the likelihood evaluated at the MLE.

Or, in other words, the standard errors for the estimates are the square root of the diagonal of the inverse observed Fisher Information.

Since we minimized the negative log likelihood, the Hessian returned by `optim()` **is** the observed Fisher information matrix.

```{r, warning = FALSE}
(mle <- optim(par = c(0.6, -0.2, 2, 2, 1), fn = nllhood_mix, x = x, method = "BFGS",
  hessian = TRUE))
mle$hessian %>% solve() %>% diag() %>% sqrt()
```

```{r}
data.frame(
  est = mle$par, 
  se = mle$hessian %>% solve() %>% diag() %>% sqrt()
)
```

## Bounds on parameters

Use `method = "L-BFGS-B"` and specify `lower` and `upper`.

# Nelder Mead

`optim()` with default method.

No need for derivatives.

**Idea**: Evaluate function at a special arrangement of points (a simplex), then consider possible changes to the arrangement:

* Reflection
* Expansion
* Contraction
* Shrink


Nice animation: https://www.benfrederickson.com/numerical-optimization/




